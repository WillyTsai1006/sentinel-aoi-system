# use this script to unzip archive.zip in Google Colab
# 1. å¼·åˆ¶ä¿®å¾©è·¯å¾‘å•é¡Œ (è§£æ±º shell-init éŒ¯èª¤)
import os
try:
    os.chdir('/content')
    print("âœ… å·²å°‡å·¥ä½œç›®éŒ„é‡ç½®ç‚º: /content")
except Exception as e:
    print(f"âš ï¸ ç„¡æ³•é‡ç½®ç›®éŒ„: {e}")
# 2. å®‰è£ YOLO (å¦‚æœä½ é‚„æ²’è£)
# !pip install ultralytics
from ultralytics import YOLO
# 3. é™¤éŒ¯ï¼šåˆ—å‡ºç•¶å‰ç›®éŒ„ä¸‹çš„æª”æ¡ˆ (ç¢ºèª archive.zip çœŸçš„åœ¨)
print("\nğŸ“‚ ç•¶å‰ç›®éŒ„ä¸‹çš„æª”æ¡ˆ:")
# !ls -lh
# 4. åŸ·è¡Œè§£å£“ç¸®
zip_file = 'archive.zip'
dataset_dir = './datasets'
if os.path.exists(zip_file):
    print(f"\nFound {zip_file}. Unzipping to {dataset_dir}...")
    # -o ä»£è¡¨ overwrite (è¦†è“‹ä¸è©¢å•), -q ä»£è¡¨å®‰éœæ¨¡å¼
    # !unzip -o -q {zip_file} -d {dataset_dir}
    print("âœ… Unzipping complete.")
    # æª¢æŸ¥è§£å£“ç¸®å¾Œçš„çµæ§‹
    if os.path.exists(dataset_dir):
        print(f"è§£å£“ç¸®å¾Œçš„è³‡æ–™å¤¾å…§å®¹ ({dataset_dir}):")
        print(os.listdir(dataset_dir))
else:
    print(f"\nâŒ Error: {zip_file} not found in /content/.")
    print("è«‹ç¢ºèªï¼š")
    print("1. ä½ æ˜¯å¦å·²ç¶“å°‡ archive.zip æ‹–æ›³åˆ°å·¦å´çš„æª”æ¡ˆæ¬„ï¼Ÿ")
    print("2. ä¸Šå‚³é€²åº¦æ¢æ˜¯å¦å·²ç¶“è·‘å®Œï¼Ÿ(Colab ä¸Šå‚³å¤§æª”æœ‰æ™‚æœƒæ…¢)")

# change xml annotations to yolo format for NEU-DET dataset 
import os
import glob
import xml.etree.ElementTree as ET
import shutil
import random
from tqdm import tqdm
# è¨­å®šå€ 
# 1. å®šç¾© NEU-DET çš„é¡åˆ¥åç¨± (å¿…é ˆä¾ç…§é †åºï¼Œé€™å¾ˆé‡è¦)
CLASSES = ['crazing', 'inclusion', 'patches', 'pitted_surface', 'rolled-in_scale', 'scratches']
# 2. åŸå§‹è³‡æ–™çš„è·¯å¾‘ (è«‹ä¿®æ”¹ç‚ºä½ è§£å£“ç¸®å¾Œçš„æ ¹ç›®éŒ„)
# å‡è¨­ä½ è§£å£“ç¸®åœ¨ /content/datasets/neu_detï¼Œç¨‹å¼æœƒè‡ªå‹•éè¿´æœå°‹è£¡é¢çš„ XML å’Œåœ–ç‰‡
SOURCE_ROOT = '/content/datasets/NEU-DET'
# 3. è¼¸å‡ºç›®æ¨™è·¯å¾‘ (æˆ‘å€‘æœƒæŠŠè½‰æ›å¥½çš„è³‡æ–™æ”¾åœ¨é€™è£¡)
OUTPUT_ROOT = '/content/datasets/neu_det_yolo'
# 4. åˆ‡åˆ†æ¯”ä¾‹ (Train : Valid)
TRAIN_RATIO = 0.8
def convert_box(size, box):
    """ å°‡ XML çš„ (xmin, xmax, ymin, ymax) è½‰æ›ç‚º YOLO çš„ (x, y, w, h) """
    dw = 1. / size[0]
    dh = 1. / size[1]
    x = (box[0] + box[1]) / 2.0
    y = (box[2] + box[3]) / 2.0
    w = box[1] - box[0]
    h = box[3] - box[2]
    return (x * dw, y * dh, w * dw, h * dh)
def convert_annotation(xml_file, output_path):
    """ è®€å–ä¸€å€‹ XML ä¸¦è½‰å­˜ç‚º TXT """
    tree = ET.parse(xml_file)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
    # æŸäº›è³‡æ–™é›†çš„ size æ˜¯ 0 (å£è³‡æ–™)ï¼Œé˜²å‘†è™•ç†
    if w == 0 or h == 0:
        return False
    out_file = open(output_path, 'w')
    has_obj = False
    for obj in root.iter('object'):
        difficult = obj.find('difficult').text
        cls = obj.find('name').text
        if cls not in CLASSES or int(difficult) == 1:
            continue
        cls_id = CLASSES.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text), float(xmlbox.find('ymax').text))
        bb = convert_box((w, h), b)
        out_file.write(f"{cls_id} {bb[0]:.6f} {bb[1]:.6f} {bb[2]:.6f} {bb[3]:.6f}\n")
        has_obj = True
    out_file.close()
    return has_obj
# ä¸»ç¨‹å¼é‚è¼¯
print("ğŸš€ é–‹å§‹ ETL è³‡æ–™è½‰æ›æµç¨‹...")
# å»ºç«‹è¼¸å‡ºè³‡æ–™å¤¾çµæ§‹
for split in ['train', 'valid']:
    os.makedirs(os.path.join(OUTPUT_ROOT, split, 'images'), exist_ok=True)
    os.makedirs(os.path.join(OUTPUT_ROOT, split, 'labels'), exist_ok=True)
# æœå°‹æ‰€æœ‰ XML æª”æ¡ˆ
xml_files = glob.glob(os.path.join(SOURCE_ROOT, '**/*.xml'), recursive=True)
print(f"ğŸ” æ‰¾åˆ° {len(xml_files)} å€‹ XML æ¨™è¨»æª”")
# éš¨æ©Ÿæ‰“äº‚
random.shuffle(xml_files)
# åˆ‡åˆ†è³‡æ–™
split_index = int(len(xml_files) * TRAIN_RATIO)
train_files = xml_files[:split_index]
valid_files = xml_files[split_index:]
print(f"ğŸ“Š è¨“ç·´é›†: {len(train_files)} å¼µ, é©—è­‰é›†: {len(valid_files)} å¼µ")
def process_files(files_list, split_name):
    for xml_path in tqdm(files_list, desc=f"Processing {split_name}"):
        # 1. æ‰¾å‡ºå°æ‡‰çš„åœ–ç‰‡è·¯å¾‘
        # NEU-DET çš„åœ–ç‰‡é€šå¸¸è·Ÿ xml åŒåï¼Œåªæ˜¯å‰¯æª”åæ˜¯ .jpg æˆ– .BMP
        img_path_jpg = xml_path.replace('.xml', '.jpg')
        img_path_bmp = xml_path.replace('.xml', '.BMP') # æœ‰äº›è³‡æ–™é›†æ˜¯ BMP
        img_path_jpeg = xml_path.replace('.xml', '.jpeg')
        # æª¢æŸ¥å“ªå€‹åœ–ç‰‡å­˜åœ¨
        if os.path.exists(img_path_jpg):
            src_img = img_path_jpg
            ext = '.jpg'
        elif os.path.exists(img_path_bmp):
            src_img = img_path_bmp
            ext = '.bmp'
        elif os.path.exists(img_path_jpeg):
            src_img = img_path_jpeg
            ext = '.jpeg'
        else:
            # å¦‚æœ xml å’Œåœ–ç‰‡åˆ†é–‹åœ¨ä¸åŒè³‡æ–™å¤¾ (ä¾‹å¦‚ XMLåœ¨ annotations, åœ–ç‰‡åœ¨ images)
            # é€™è£¡åšä¸€å€‹ç°¡å–®çš„è·¯å¾‘æ›¿æ›å˜—è©¦
            base_name = os.path.basename(xml_path).replace('.xml', '')
            # å˜—è©¦æœå°‹åœ–ç‰‡
            possible_imgs = glob.glob(os.path.join(SOURCE_ROOT, '**', f"{base_name}.*"), recursive=True)
            # éæ¿¾æ‰ xml æœ¬èº«
            possible_imgs = [p for p in possible_imgs if not p.endswith('.xml')]
            if len(possible_imgs) > 0:
                src_img = possible_imgs[0]
                ext = os.path.splitext(src_img)[1]
            else:
                print(f"âš ï¸ æ‰¾ä¸åˆ°åœ–ç‰‡: {xml_path}")
                continue
        # 2. å®šç¾©è¼¸å‡ºè·¯å¾‘
        file_name = os.path.basename(xml_path).replace('.xml', '')
        dst_img_path = os.path.join(OUTPUT_ROOT, split_name, 'images', f"{file_name}{ext}")
        dst_txt_path = os.path.join(OUTPUT_ROOT, split_name, 'labels', f"{file_name}.txt")
        # 3. è½‰æ› XML -> TXT
        if convert_annotation(xml_path, dst_txt_path):
            # 4. åªæœ‰ç•¶æ¨™è¨»æˆåŠŸè½‰æ›ï¼Œæ‰è¤‡è£½åœ–ç‰‡
            shutil.copy(src_img, dst_img_path)
process_files(train_files, 'train')
process_files(valid_files, 'valid')
print(f"\nâœ… è½‰æ›å®Œæˆï¼æ–°è³‡æ–™é›†ä½æ–¼: {OUTPUT_ROOT}")

# ç”¢ç”Ÿ data.yaml æª”æ¡ˆä¾› YOLO è¨“ç·´ä½¿ç”¨
import yaml
yaml_content = {
    'path': '/content/datasets/neu_det_yolo', # æŒ‡å‘å‰›å‰›è½‰æ›å¥½çš„æ–°ç›®éŒ„
    'train': 'train/images',
    'val': 'valid/images',
    'names': {
        0: 'crazing',
        1: 'inclusion',
        2: 'patches',
        3: 'pitted_surface',
        4: 'rolled-in_scale',
        5: 'scratches'
    }
}
with open('/content/datasets/neu_det_yolo/data.yaml', 'w') as f:
    yaml.dump(yaml_content, f)
print("âœ… data.yaml è¨­å®šå®Œæˆ")

# train yolo model on neu_det dataset
from ultralytics import YOLO
# è¼‰å…¥ Medium æ¨¡å‹
model = YOLO('yolov8m.pt')
print("ğŸš€ é–‹å§‹æ­£å¼è¨“ç·´...")
results = model.train(
    data='/content/datasets/neu_det_yolo/data.yaml', # ä½¿ç”¨æ–°ç”Ÿæˆçš„ yaml
    epochs=50,
    imgsz=640,
    batch=16,
    patience=10,
    name='sentinel_aoi_final',
    augment=True
)